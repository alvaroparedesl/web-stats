---
output:
  pdf_document: default
  html_document: default
---
# Probabilidad {#prob-2}

Intuitivas en parte, no tanto en varias. Todo es posible, pero a la vez imposible. Sí, de eso se tratan las probabilidades.

La probabilidad no se puede dar por definición, se deriva de axiomas (que veremos en \@ref(tconjuntos-2)) en base a una función de probabilidad. Cualquier cosa que cumpla con los estos 3 axiomas, es una probabilidad. La definición filosófica, la dejamos para otro momento, por ahora nos quedaremos con la definición matemática.


## Conceptos importantes {#cimportantes-2}


### Muestras significativas {#msignificativas-2}

Tal y como nos referimos en \@ref(cbasicos-1), una muestra no puede ser calificada de *"significativa"*. Como la mayoría de las veces desconocemos las características que nos interesan de la población, es imposible saber si dicha muestra es o no es significativa. Por lo tanto el concepto es absurdo. Lo que sí es necesario de una muestra, es que su origen debe ser probabilístico, lo que se traduce en que debe ser posible asignarle una probabilidad de ser seleccionada (probabilidad de obtener dicha muestra).

¿De qué tamaño debe ser la muestra? Depende de la variabilidad de la población. Pero diantres, si la población es desconocida, ¿cómo sabemos todas esas cosas? Bueno, bienvenido a los supuestos y simplificaciones, que de esos tendremos por montón. Lo bueno, es que seremos capaces de cuantificarlos.

### Sesgo

La cualidad de sesgado o insesgado es relativa al estimador, no a la muestra.


## Teoría de conjuntos {#tconjuntos-2}

* Contenido: $A \subset B$
* Igualdad: $A = B \Leftrightarrow A \subset B y B \subset A$
* Unión: $A \cup B$
* Intersección: $A \cap B$
* Complemento: $A^c$

### Teoremas

1. Conmutatividad: $A \cup B = B \cup A$ y $A \cap B = B \cap A$
2. Asociatividad: $A \cup (B \cup C) = (A \cup B) \cup C$ y $A \cap (B \cap C) = (A \cap B) \cap C$
3. Distributiva: $A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$ y $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$
4. Complemento: $(A \cup B)^c = (A^c \cap B^c)$ y $(A \cap B)^c = (A^c \cup B^c)$


## Teoría de probabilidad

Debemos definir:

1. Espacio muestral: conjunto de todos los resultados posibles ($\Omega$)
2. Familia de eventos $\Im$ ($\sigma$-álgebra o *Borel field*): subconjunto de $\Omega$ a los cuales les asigno una probabilidad ($P$). Cobra importancia e interés, cuando $\Omega$ no es numerable o es infinito. En el fondo es una partición de $\Omega$ que puede contener la misma cantidad de elementos (partición fina, caso finito o numerable) o menos elementos (partición gruesa, para casos no numerables o infinitos).
3. Probabilidad $P$: definición de función de probabilidad


### Características de $\Im$

Para que $\Im$ sea considerada una $\omega$-álgebra, debe tener las siguientes propiedades:

1. $\emptyset \in \Im$
2. Si $A \in \Im$, entonces $A^c \in \Im$
3. Si $A_1, A_2, ... \in \Im 	\Rightarrow \bigcup_{i=1}^\infty A_i \in \Im$

Cualquier cosa que cumpla estas propiedades, es una $\sigma$-álgebra. Una $\sigma$-álgebra contiene $2^n$ conjuntos, donde $n$ es la cantidad de elementos contenidos.


### Características de una función de probabilidad $P$

Definida como:
$P: \Im \rightarrow \mathbb{R} \\$ 
$A \rightarrow P(A)$

Y dado un espacio muestral $\Omega$ y una $\Im$ asociada a dicho espacio muestral, $P$ es una probabilidad de función (con dominio en $\Im$) si cumple los siguientes 3 axiomas de probabilidad:

1. $P(A) \ge 0, \forall A \in \Im$
2. $P(\Omega) = 1$
3. Si $A_1, A_2, ... \in \Im$ y son pares disjuntos, entonces $P(\bigcup_{i=1}^\infty A_i) = \sum_{i=1}^\infty P(A_i)$


Si $P$ es una función de probabilidad, con $A \in \Im$, entonces se cumple el siguiente teorema:

1. $P(\emptyset) = 0$
2. $P(A) \leq 1$
3. $P(A^c) = 1- P(A)$


Si $P$ es una función de probabilidad, con $A,B \in \Im$, entonces se cumple el siguiente teorema:

1. $P(B \cap A^c) = P(B) - P(A \cap B)$
2. $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
3. Si $A \subset B \Rightarrow P(A) \leq P(B)$


En el segundo caso, cuando los eventos son disjuntos, $A \cap B = \emptyset$


### Probabilidades en poblaciones finitas

Y en donde se pueden contar los elementos; también puede ser el caso de poblaciones ¿infinitas?, pero elementos contables (como los números enteros).


### Permutaciones y combinaciones

Tenemos:

* Permutaciones: Arreglo ordenado de objetos, con o sin reposición/reemplazo. Es decir, el orden si importa; elegir entre dos letras $A$ y $B$, son elementos distintos $AB$ y $BA$.
* Combinaciones: Arreglo no ordenado de objetos, con o sin reposición/reemplazo. El orden no importa; elegir entre dos letras $A$ y $B$, son el mismo elemento $AB$ y $BA$.

En el caso equipobrable (cada evento tiene probabilida de ocurrencia $1/n$), se tiene lo siguiente, siendo $r$ el tamaño de la muestra y $n$ un conjunto dado:

```{r permutaciones-combinaciones-equi-2, echo = FALSE, message = FALSE, warning=FALSE}
# , fig.cap = 'A figure caption.'
mdt <- matrix(c('$n^r$', 
                '$\\frac{(n + r - 1)!}{r!(n-r)!} = \\binom{n + r - 1}{r}$',
                '$\\frac{n!}{(n-r)!}$', 
                '$\\frac{n!}{r!(n-r)!} = \\binom{n}{r}$'), 
              nrow=2, dimnames=list(c("Ordenado (Permutaciones)", "No ordenado (Combinaciones)"), c("Con reemplazo", "Sin reemplazo")))
knitr::kable(mdt, 'html', caption = 'Cuadro resumen para contar eventos ordenados o no ordenados, con o sin reposición.', booktabs = TRUE, align = "c")
```

Como se puede apreciar en la tabla \@ref(tab:permutaciones-combinaciones-equi-2), las combinaciones tienen una notación especial denominada por $\binom{}{}$, que es una abreviatura para dicha formulación. Si pensamos las cuatro expresiones que aparecen, podemos deducir que ante un mismo evento, las permutaciones (donde el orden importa) entregarán un número mayor de eventos posibles, con respecto a las combinaciones. Por otro lado, cuando hay reposición, el número de eventos que se pueden generar también es superior a cuando no hay reposición. Son ideas un poco obvias e intuitivas, pero que vale la pena resaltar.

Veamos un ejemplo de cada caso y como se llega a las formulas resumen de la tabla \@ref(tab:permutaciones-combinaciones-equi-2), partiendo de la pregunta:

*Si tengo 6 bolas de colores ¿de cuántas maneras puedo tomar 3 bolas?*

1. Permutación con reposición: Si antes de tomar la siguiente bola, devuelvo la que saqué previamente. La primera vez, puedo sacar 6 bolas, la segunda vez también puedo sacar 6 bolas pues he repuesto la que saqué anteriormente y la tercera vez, también puedo tomar 6 bolas. Esto es equivalente a $6 * 6 * 6 = 6^3 = 216$. Es decir, puedo hacerlo de 216 formas posibles.

2. Permutación sin reposición: Si repito el ejercicio el anterior, sin devolver las bolas que voy sacando, obtengo el resultado de multiplicar $6 * 5 * 4 = 120$. Esta operación, la podemos anotar como $\frac{n!}{(n-r)!} = \frac{12!}{(12-3)!} = \frac{6!}{3!} = \frac{6*5*4*3!}{3!}$. Cancelando $3!$, obtenemos la multiplicación deseada. Notar que de forma intuitiva, no podemos obtener un resultado mayor al experimento con reposición, pues cada vez vamos teniendo menos opciones. 

3. Combinación sin reposición: Similar al de permutación con reposición, pero ahora hay que descontar las veces en que la elección tenía los mismos elemntos, pero ordenados de forma diferente. Esto sería $\binom{n}{r} = \frac{n!}{r!(n-r)!} = \frac{6!}{3!(6-3)!} = \frac{6*5*4}{3!} = 20$. En este caso, el $3!$ está contando de cuantas maneras se pueden elegir esas 3 bolas ordenadamente sin reposición y como está en el denominador, estamos descontando dichas combinaciones del espacio de solución. Nuevamente es bueno notar que el resultado esperado, también es un número menor al de una permutación sin reposición (y en realidad, debiera ser siempre el menor de los cuatro casos), pues el orden esta vez no nos interesa, y con ello caen automáticamente el número de casos.

4. Combinación con reposición: Este no es muy similar a nada, y es un poco complejo de explicar. Es mejor ver primero la aplicación, que sería $\binom{n + r - 1}{r} = \frac{(n + r - 1)!}{r!((n + r - 1) - r)!} = \frac{(6 + 3 - 1)!}{3!((6 + 3 - 1) - 3)!} = \frac{8*7*6*5!}{3! 5!} = 56$. El término $r$ queda eliminado en el denominador, con lo que en el fondo queda $\frac{(n + r - 1)!}{r!(n - 1)!}$. Es de esperar que al ser con reposición, se obtenga un número superior a sin reposición, y obviamente, por debajo de al menos el caso de permutación con reposición.


### Probabilidad condicional e independencia

Cuando se quiere conocer la probabilidad de que se produzca un evento $A$ dado que ocurrió un evento $B$, hablamos de probabilidad condicional. Esto está expresado en la ecuación \@ref(eq:probabilidad-condicional).

\begin{equation}
P(A|B) = \frac{P(A \cap B)}{P(B)}
(\#eq:probabilidad-condicional)
\end{equation}

Si la probabilidad de que ocurra $P(A|B)$ es la misma que $P(A)$, entonces el evento $A$ es independiente de $B$. Notar que la ecuación \@ref(eq:probabilidad-condicional) es equivalente a la expresion \@ref(eq:probabilidad-condicional-ext), gracias a las propiedades de simetría.

\begin{equation}
P(A \cap B) = P(A|B) * P(B) = P(B|A) * P(A)
(\#eq:probabilidad-condicional-ext)
\end{equation}

Por otro lado, si $A$ es independiente de $B$ podemos resolver como se muestra en la ecuación \@ref(eq:eventos-independientes-desarrollo).

\begin{equation}
P(A|B) = \frac{P(B|A) * P(A)}{P(B)} \\
= P(A|B) \frac{P(A)} {P(B)} \\
= P(B) \frac{P(A)} {P(B)} \\
= P(A)
(\#eq:eventos-independientes-desarrollo)
\end{equation}

La expresión \@ref(eq:eventos-independientes-desarrollo) se derivda entonces, en la expresión \@ref(eq:eventos-independientes).

\begin{equation}
P(A|B) * P(B) = P(A \cap B) = P(A) * P(B)
(\#eq:eventos-independientes)
\end{equation}

#### Probabilidad total

\begin{equation}
\sum_{i=1}^nP(A_i)P(B|A_i)
(\#eq:probabilidad-total)
\end{equation}

#### Bayes

\begin{equation}
P(A_k|B) = \frac{P(A_k)P(B|A_k)}{\sum_{i=1}^nP(A_i)P(B|A_i)}, k=1,...,n
(\#eq:teorema-bayes)
\end{equation}

### Variable aleatoria


### Funciones de distribución